{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBhDRW7ZgJKB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Loading (ETL)\n",
        "\n",
        "Q1. Data Understanding\n",
        "\n",
        "Identify all data quality issues present in the dataset that can cause problems during data loading.\n",
        "\n",
        "Ans:Issues present in dataset:\n",
        "\n",
        "- Duplicate Order_ID → O104 appears twice\n",
        "\n",
        "- Missing value → Sales_Amount is NULL in one row\n",
        "\n",
        "- Invalid data type → “Three Thousand” stored as text instead of number\n",
        "\n",
        "- Inconsistent date formats → multiple date styles used\n",
        "\n",
        "Q2. Primary Key Validation\n",
        "\n",
        "Assume ORDER_ID is the Primary Key.\n",
        "\n",
        "a) Is the dataset violating the Primary Key rule?\n",
        "\n",
        "b) Which record(s) cause this violation?\n",
        "\n",
        "Ans: The validation of primary keys and order id are:\n",
        "\n",
        "a) Primary Key = Order_ID\n",
        "Because it should uniquely identify each order.\n",
        "\n",
        "b) Violation = Duplicate key value\n",
        "O104 appears more than once → breaks uniqueness rule.\n",
        "\n",
        "Q3. Missing Value Analysis\n",
        "\n",
        "Which column(s) contain missing values?\n",
        "\n",
        "a) List the affected records\n",
        "\n",
        "b) Explain why loading these records without handling missing values is risky.\n",
        "\n",
        "Ans: We have analysis the missing value\n",
        "\n",
        "a) Column with missing value → Sales_Amount\n",
        "\n",
        "b) Risk if loaded without fixing:\n",
        "\n",
        "- Wrong revenue calculations\n",
        "\n",
        "- Incorrect reports & dashboards\n",
        "\n",
        "- Aggregations like SUM/AVG become misleading\n",
        "\n",
        "Q4. Data Type Validation\n",
        "\n",
        "Identify records where Sales_Amount violates expected data type rules.\n",
        "\n",
        "a) Which record(s) will fail numeric validation?\n",
        "\n",
        "b) What would happen if this dataset is loaded into a SQL table with Sales_amount as DECIMAL?\n",
        "\n",
        "Ans: We have to do data type validation are\n",
        "\n",
        "a) Violations:\n",
        "\n",
        "Sales_Amount has text value → “Three Thousand”\n",
        "\n",
        "Should be numeric but stored as string\n",
        "\n",
        "b) Correct data type → DECIMAL(10,2) (or FLOAT/NUMERIC)\n",
        "\n",
        "Q5. Date Format Consistency\n",
        "\n",
        "The column has multiple formats.\n",
        "\n",
        "a) List all date formats present in the dataset\n",
        "\n",
        "b) Why is this a problem during data loading?\n",
        "\n",
        "Ans: We  have to formate consistency are\n",
        "\n",
        "Problem: Dates are stored in multiple formats →\n",
        "Examples: 12-01-2024, 15-01-2024, 25-01-2024 (could be DD-MM or MM-DD confusion)\n",
        "\n",
        "Standardize to: YYYY-MM-DD format\n",
        "Example → 2024-01-12\n",
        "\n",
        "Q6. Load Readiness Decision\n",
        "\n",
        "Based on the dataset condition:\n",
        "\n",
        "a) Should this dataset be loaded directly into the database? (Yes/No)\n",
        "\n",
        "b) Justify your answer with at least three reasons\n",
        "\n",
        "Ans: We have to load readiness decision based on datasets conditions are;\n",
        "\n",
        "a) Dataset ready to load? → NO\n",
        "\n",
        "b) Reasons (any 3):\n",
        "\n",
        "- Duplicate primary key values\n",
        "\n",
        "- Missing Sales_Amount\n",
        "\n",
        "- Non-numeric value in numeric column\n",
        "\n",
        "- Inconsistent date format.\n",
        "\n",
        "Q7. Pre-Load Validation Checklist\n",
        "\n",
        "List the exact pre-load validation checks you would perform on this dataset before loading.\n",
        "\n",
        "Ans:Steps before loading:\n",
        "\n",
        "- Check primary key uniqueness\n",
        "\n",
        "- Validate numeric columns\n",
        "\n",
        "- Check NULL values\n",
        "\n",
        "- Standardize date format\n",
        "\n",
        "- Remove duplicates\n",
        "\n",
        "- Verify column data types\n",
        "\n",
        "- Range checks (Sales_Amount > 0)\n",
        "\n",
        "Q8. Cleaning Strategy\n",
        "\n",
        "Describe the step-by-step cleaning actions required to make this dataset load-ready.\n",
        "\n",
        "Ans:Steps to make load-ready:\n",
        "\n",
        "- Remove or fix duplicate Order_ID rows\n",
        "\n",
        "- Convert “Three Thousand” → 3000\n",
        "\n",
        "- Fill or impute missing Sales_Amount\n",
        "\n",
        "- Convert Sales_Amount to DECIMAL\n",
        "\n",
        "- Standardize all dates to one format\n",
        "\n",
        "- Validate all records again\n",
        "\n",
        "Q9. Loading Strategy Selection\n",
        "\n",
        "Assume this dataset represents daily sales data.\n",
        "\n",
        "a) Should a Full Load or Incremental Load be used?\n",
        "\n",
        "b) Justify your choice.\n",
        "\n",
        "Ans:The load strategy selection are\n",
        "\n",
        "a) Best method → Full Load\n",
        "\n",
        "b) Why:\n",
        "\n",
        "- Small dataset\n",
        "\n",
        "- Many quality issues → easier to reload clean data\n",
        "\n",
        "- No historical tracking required here\n",
        "\n",
        "Q10. BI Impact Scenario\n",
        "\n",
        "Assume this dataset was loaded without cleaning and connected to a BI dashboard.\n",
        "\n",
        "a) What incorrect results might appear in Total Sales KPI?\n",
        "\n",
        "b) Which records specifically would cause misleading insights?\n",
        "\n",
        "c) Why would BI tools not detect these issues automatically?\n",
        "\n",
        "Ans:We assuming this dataset is not cleaning\n",
        "\n",
        "a) What incorrect results might appear in Total Sales KPI?\n",
        "\n",
        "- Total Sales would be wrong (lower or inconsistent) because:\n",
        "\n",
        "- One record has NULL Sales_Amount → not counted in SUM\n",
        "\n",
        "- One record has text value (“Three Thousand”) → ignored or treated as 0\n",
        "\n",
        "- Duplicate Order_ID record may cause double counting\n",
        "So KPI may show under-reported or inflated total sales.\n",
        "\n",
        "b) Which records specifically would cause misleading insights?\n",
        "\n",
        "- The record with NULL Sales_Amount → reduces total incorrectly\n",
        "\n",
        "- The record with “Three Thousand” in Sales_Amount → not added to totals\n",
        "\n",
        "- The duplicate Order_ID (O104) rows → may double count that order’s sales\n",
        "\n",
        "- These rows directly distort revenue metrics.\n",
        "\n",
        "c) Why would BI tools not detect these issues automatically?\n",
        "\n",
        "- BI tools generally trust the loaded data schema\n",
        "\n",
        "- They perform aggregation (SUM/AVG) but don’t enforce primary key uniqueness\n",
        "\n",
        "- Text/NULL values are usually silently ignored in numeric calculations\n",
        "\n",
        "- Data quality validation is expected to happen in ETL stage, not BI layer."
      ],
      "metadata": {
        "id": "5feTLGPTgLOA"
      }
    }
  ]
}